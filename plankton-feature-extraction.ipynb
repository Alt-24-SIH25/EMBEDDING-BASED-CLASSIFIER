{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2331456,"sourceType":"datasetVersion","datasetId":1407240},{"sourceId":8815412,"sourceType":"datasetVersion","datasetId":5302658}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:37:47.328432Z","iopub.execute_input":"2025-09-30T04:37:47.328775Z","iopub.status.idle":"2025-09-30T04:38:29.068727Z","shell.execute_reply.started":"2025-09-30T04:37:47.328750Z","shell.execute_reply":"2025-09-30T04:38:29.067531Z"}},"outputs":[{"name":"stderr","text":"2025-09-30 04:38:07.267177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759207087.595925      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759207087.688235      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ---------------------------\n# Configuration\n# ---------------------------\nCONFIG = {\n    'num_reference_images': 50, \n    'embedding_method': 'mean',\n    'augment_reference': True,\n    'augment_query': True, \n    'image_preprocessing': True,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:38:29.071461Z","iopub.execute_input":"2025-09-30T04:38:29.072095Z","iopub.status.idle":"2025-09-30T04:38:29.077474Z","shell.execute_reply.started":"2025-09-30T04:38:29.072060Z","shell.execute_reply":"2025-09-30T04:38:29.076433Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ---------------------------\n# Step 1. Setup paths and explore dataset\n# ---------------------------\ndataset_path = Path(\"/kaggle/input/whoi-plankton-2014/2014\")\n\n# List species folders\nspecies_folders = [p for p in dataset_path.iterdir() if p.is_dir()]\nprint(f\"Number of species: {len(species_folders)}\")\nprint(f\"Example species: {[f.name for f in species_folders[:5]]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:38:29.078377Z","iopub.execute_input":"2025-09-30T04:38:29.078847Z","iopub.status.idle":"2025-09-30T04:38:29.226974Z","shell.execute_reply.started":"2025-09-30T04:38:29.078822Z","shell.execute_reply":"2025-09-30T04:38:29.225976Z"}},"outputs":[{"name":"stdout","text":"Number of species: 94\nExample species: ['DactFragCerataul', 'Rhizosolenia', 'Chaetoceros', 'bead', 'G_delicatula_external_parasite']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ---------------------------\n# Step 2. Image Preprocessing Functions\n# ---------------------------\ndef preprocess_plankton_image(pil_img: Image.Image, target_size=224) -> Image.Image:\n    \"\"\"\n    Preprocess plankton images for better CLIP performance\n    - Resize while maintaining aspect ratio\n    - Add padding to make square\n    - Enhance contrast\n    \"\"\"\n    # Convert to RGB\n    img = pil_img.convert(\"RGB\")\n    \n    # Get original size\n    orig_w, orig_h = img.size\n    \n    # Calculate scaling to fit within target_size while maintaining aspect ratio\n    scale = min(target_size / orig_w, target_size / orig_h)\n    new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n    \n    # Resize image\n    img = img.resize((new_w, new_h), Image.LANCZOS)\n    \n    # Create white background (plankton images often have white/light backgrounds)\n    background = Image.new('RGB', (target_size, target_size), (255, 255, 255))\n    \n    # Paste resized image in center\n    offset = ((target_size - new_w) // 2, (target_size - new_h) // 2)\n    background.paste(img, offset)\n    \n    return background\n\n\ndef apply_augmentation(pil_img: Image.Image) -> list:\n    \"\"\"\n    Apply augmentations to generate multiple views of the same image\n    Returns list of augmented images\n    \"\"\"\n    augmented = [pil_img]  # Original\n    \n    # Horizontal flip\n    augmented.append(pil_img.transpose(Image.FLIP_LEFT_RIGHT))\n    \n    # Vertical flip\n    augmented.append(pil_img.transpose(Image.FLIP_TOP_BOTTOM))\n    \n    # Slight rotations\n    augmented.append(pil_img.rotate(90, expand=True))\n    augmented.append(pil_img.rotate(180, expand=True))\n    augmented.append(pil_img.rotate(270, expand=True))\n    \n    return augmented","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:38:29.227703Z","iopub.execute_input":"2025-09-30T04:38:29.228037Z","iopub.status.idle":"2025-09-30T04:38:29.239648Z","shell.execute_reply.started":"2025-09-30T04:38:29.228008Z","shell.execute_reply":"2025-09-30T04:38:29.238489Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ---------------------------\n# Step 3. CLIP setup\n# ---------------------------\nmodel_path = \"/kaggle/input/openaiclip-vit-base-patch32\"\n\nprint(\"Loading CLIP model...\")\nclip_model = CLIPModel.from_pretrained(model_path)\nclip_processor = CLIPProcessor.from_pretrained(model_path)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model = clip_model.to(DEVICE)\nprint(f\"Using device: {DEVICE}\")\n\n@torch.no_grad()\ndef image_embedding(pil_img: Image.Image) -> np.ndarray:\n    \"\"\"Extract CLIP embedding from PIL image\"\"\"\n    inputs = clip_processor(images=pil_img, return_tensors=\"pt\").to(DEVICE)\n    feats = clip_model.get_image_features(**inputs).cpu().numpy()\n    feats = feats / np.linalg.norm(feats, axis=1, keepdims=True)\n    return feats[0]\n\n@torch.no_grad()\ndef batch_image_embeddings(pil_images: list) -> np.ndarray:\n    \"\"\"Extract CLIP embeddings for multiple images at once (faster)\"\"\"\n    inputs = clip_processor(images=pil_images, return_tensors=\"pt\").to(DEVICE)\n    feats = clip_model.get_image_features(**inputs).cpu().numpy()\n    feats = feats / np.linalg.norm(feats, axis=1, keepdims=True)\n    return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:38:29.240836Z","iopub.execute_input":"2025-09-30T04:38:29.241219Z","iopub.status.idle":"2025-09-30T04:38:34.293919Z","shell.execute_reply.started":"2025-09-30T04:38:29.241187Z","shell.execute_reply":"2025-09-30T04:38:34.292532Z"}},"outputs":[{"name":"stdout","text":"Loading CLIP model...\n","output_type":"stream"},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ---------------------------\n# Step 4. Build IMPROVED reference dataset\n# ---------------------------\nprint(f\"Using {CONFIG['num_reference_images']} images per species\")\nprint(f\"Augmentation: {CONFIG['augment_reference']}\")\nprint(f\"Preprocessing: {CONFIG['image_preprocessing']}\")\n\nspecies_embeddings = {}\nrecords = []\n\nfor species_folder in tqdm(sorted(dataset_path.iterdir()), desc=\"Processing species\"):\n    if not species_folder.is_dir():\n        continue\n    \n    species_name = species_folder.name\n    png_files = list(species_folder.glob(\"*.png\"))\n    \n    if not png_files:\n        print(f\"Warning: No PNG files found in {species_name}\")\n        continue\n    \n    # Select multiple reference images\n    num_refs = min(CONFIG['num_reference_images'], len(png_files))\n    \n    # Distribute selection across the dataset (not just first N)\n    if num_refs >= len(png_files):\n        # Use all images if we have fewer than requested\n        selected_files = png_files\n    else:\n        # Sample evenly distributed images\n        step = max(1, len(png_files) // num_refs)\n        selected_files = png_files[::step][:num_refs]\n    \n    all_embeddings = []\n    \n    try:\n        for img_path in selected_files:\n            img = Image.open(img_path).convert(\"RGB\")\n            \n            # Apply preprocessing\n            if CONFIG['image_preprocessing']:\n                img = preprocess_plankton_image(img)\n            \n            # Apply augmentation if enabled\n            if CONFIG['augment_reference']:\n                augmented_images = apply_augmentation(img)\n                # Get embeddings for all augmented versions\n                embs = batch_image_embeddings(augmented_images)\n                all_embeddings.extend(embs)\n            else:\n                emb = image_embedding(img)\n                all_embeddings.append(emb)\n        \n        # Aggregate embeddings \n        all_embeddings = np.array(all_embeddings)\n        \n        if CONFIG['embedding_method'] == 'mean':\n            final_embedding = np.mean(all_embeddings, axis=0)\n        else:  # median\n            final_embedding = np.median(all_embeddings, axis=0)\n        \n        # Renormalize\n        final_embedding = final_embedding / np.linalg.norm(final_embedding)\n        \n        species_embeddings[species_name] = final_embedding\n        \n        records.append({\n            \"species\": species_name,\n            \"num_reference_images\": num_refs,\n            \"num_embeddings_used\": len(all_embeddings),\n            \"total_images_available\": len(png_files),\n            \"embedding\": final_embedding\n        })\n        \n    except Exception as e:\n        print(f\"Error processing {species_name}: {e}\")\n        continue\n\ndf_ref = pd.DataFrame(records)\nprint(f\"\\nReference dataset built: {df_ref.shape}\")\nprint(f\"Species with embeddings: {len(species_embeddings)}\")\nprint(\"\\nEmbeddings statistics:\")\nprint(df_ref[['species', 'num_reference_images', 'num_embeddings_used', 'total_images_available']].head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:38:34.294822Z","iopub.execute_input":"2025-09-30T04:38:34.295198Z","iopub.status.idle":"2025-09-30T05:06:00.542387Z","shell.execute_reply.started":"2025-09-30T04:38:34.295166Z","shell.execute_reply":"2025-09-30T05:06:00.541309Z"}},"outputs":[{"name":"stdout","text":"Using 50 images per species\nAugmentation: True\nPreprocessing: True\n","output_type":"stream"},{"name":"stderr","text":"Processing species: 100%|██████████| 94/94 [27:26<00:00, 17.51s/it]","output_type":"stream"},{"name":"stdout","text":"\nReference dataset built: (94, 5)\nSpecies with embeddings: 94\n\nEmbeddings statistics:\n                          species  num_reference_images  num_embeddings_used  \\\n0                        Akashiwo                     1                    6   \n1                  Amphidinium_sp                    50                  300   \n2                Asterionellopsis                    50                  300   \n3                     Cerataulina                    50                  300   \n4          Cerataulina_flagellate                     5                   30   \n5                        Ceratium                     6                   36   \n6                     Chaetoceros                    50                  300   \n7             Chaetoceros_didymus                    11                   66   \n8  Chaetoceros_didymus_flagellate                     1                    6   \n9          Chaetoceros_flagellate                     4                   24   \n\n   total_images_available  \n0                       1  \n1                      66  \n2                     128  \n3                     412  \n4                       5  \n5                       6  \n6                    1871  \n7                      11  \n8                       1  \n9                       4  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ---------------------------\n# Step 5. Classify test images with improvements\n# ---------------------------\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Testing classification\")\nprint(\"=\"*50)\n\ntest_species_folder = species_folders[0]\ntest_images = list(test_species_folder.glob(\"*.png\"))[:10]\n\nprint(f\"\\nTest species: {test_species_folder.name}\")\nprint(f\"Found {len(test_images)} test images\")\n\nspecies_list = list(species_embeddings.keys())\nref_matrix = np.stack([species_embeddings[s] for s in species_list])\n\nresults = []\n\nfor img_path in tqdm(test_images, desc=\"Classifying images\"):\n    try:\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        # Apply preprocessing\n        if CONFIG['image_preprocessing']:\n            img = preprocess_plankton_image(img)\n        \n        # Test-time augmentation \n        if CONFIG['augment_query']:\n            augmented = apply_augmentation(img)\n            embs = batch_image_embeddings(augmented)\n            q_emb = np.mean(embs, axis=0)\n            q_emb = q_emb / np.linalg.norm(q_emb)\n        else:\n            q_emb = image_embedding(img)\n        \n        # Compute similarities\n        similarities = ref_matrix @ q_emb\n        \n        # Get predictions\n        best_idx = np.argmax(similarities)\n        best_species = species_list[best_idx]\n        best_score = similarities[best_idx]\n        \n        # Top 5 predictions\n        top5_indices = np.argsort(similarities)[-5:][::-1]\n        top5_species = [species_list[i] for i in top5_indices]\n        top5_scores = [similarities[i] for i in top5_indices]\n        \n        results.append({\n            \"query_image\": img_path.name,\n            \"true_species\": test_species_folder.name,\n            \"predicted_species\": best_species,\n            \"confidence\": float(best_score),\n            \"is_correct\": best_species == test_species_folder.name,\n            \"top5_predictions\": top5_species,\n            \"top5_scores\": [float(s) for s in top5_scores]\n        })\n        \n    except Exception as e:\n        print(f\"Error processing {img_path.name}: {e}\")\n        continue\n\ndf_results = pd.DataFrame(results)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Classification Results\")\nprint(\"=\"*50)\nprint(df_results[['query_image', 'true_species', 'predicted_species', 'confidence', 'is_correct']])\n\nif len(df_results) > 0:\n    accuracy = df_results['is_correct'].mean()\n    avg_confidence = df_results['confidence'].mean()\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"METRICS\")\n    print(f\"{'='*50}\")\n    print(f\"Accuracy: {accuracy:.2%} ({df_results['is_correct'].sum()}/{len(df_results)})\")\n    print(f\"Average confidence: {avg_confidence:.4f}\")\n    print(f\"Correct predictions avg confidence: {df_results[df_results['is_correct']]['confidence'].mean():.4f}\")\n    print(f\"Wrong predictions avg confidence: {df_results[~df_results['is_correct']]['confidence'].mean():.4f}\")\n    \n    # Top-5 accuracy\n    top5_correct = sum([row['true_species'] in row['top5_predictions'] for _, row in df_results.iterrows()])\n    top5_accuracy = top5_correct / len(df_results)\n    print(f\"Top-5 Accuracy: {top5_accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T05:06:00.544755Z","iopub.execute_input":"2025-09-30T05:06:00.545075Z","iopub.status.idle":"2025-09-30T05:06:06.417624Z","shell.execute_reply.started":"2025-09-30T05:06:00.545037Z","shell.execute_reply":"2025-09-30T05:06:06.416555Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTesting classification\n==================================================\n\nTest species: DactFragCerataul\nFound 10 test images\n","output_type":"stream"},{"name":"stderr","text":"Classifying images: 100%|██████████| 10/10 [00:05<00:00,  1.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nClassification Results\n==================================================\n                       query_image      true_species predicted_species  \\\n0  IFCB5_2014_248_004113_05455.png  DactFragCerataul      Rhizosolenia   \n1  IFCB5_2014_328_152205_07231.png  DactFragCerataul  DactFragCerataul   \n2  IFCB5_2014_315_144452_00017.png  DactFragCerataul       Skeletonema   \n3  IFCB5_2014_315_135823_06074.png  DactFragCerataul  DactFragCerataul   \n4  IFCB5_2014_315_144452_01507.png  DactFragCerataul       Skeletonema   \n5  IFCB5_2014_259_120213_05242.png  DactFragCerataul      Rhizosolenia   \n6  IFCB5_2014_002_210221_01583.png  DactFragCerataul      Rhizosolenia   \n7  IFCB5_2014_315_142115_01290.png  DactFragCerataul      Rhizosolenia   \n8  IFCB5_2014_248_010423_01317.png  DactFragCerataul  DactFragCerataul   \n9  IFCB5_2014_315_135823_07315.png  DactFragCerataul       Skeletonema   \n\n   confidence  is_correct  \n0    0.974654       False  \n1    0.972635        True  \n2    0.957793       False  \n3    0.953981        True  \n4    0.984811       False  \n5    0.966115       False  \n6    0.987965       False  \n7    0.982660       False  \n8    0.966641        True  \n9    0.980332       False  \n\n==================================================\nMETRICS\n==================================================\nAccuracy: 30.00% (3/10)\nAverage confidence: 0.9728\nCorrect predictions avg confidence: 0.9644\nWrong predictions avg confidence: 0.9763\nTop-5 Accuracy: 80.00%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ---------------------------\n# Step 6. Save results\n# ---------------------------\ndf_ref_save = df_ref.drop(columns=['embedding'])\ndf_ref_save.to_csv(\"reference_dataset_improved.csv\", index=False)\n\ndf_results_save = df_results.drop(columns=['top5_predictions', 'top5_scores'], errors='ignore')\ndf_results_save.to_csv(\"classification_results_improved.csv\", index=False)\n\nembeddings_array = np.stack([species_embeddings[s] for s in species_list])\nnp.save(\"species_embeddings_improved.npy\", embeddings_array)\nnp.save(\"species_names.npy\", np.array(species_list))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T05:06:06.418466Z","iopub.execute_input":"2025-09-30T05:06:06.418741Z","iopub.status.idle":"2025-09-30T05:06:06.435810Z","shell.execute_reply.started":"2025-09-30T05:06:06.418717Z","shell.execute_reply":"2025-09-30T05:06:06.434540Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ---------------------------\n# Step 7. Analyze Confusion Patterns\n# ---------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"CONFUSION ANALYSIS\")\nprint(\"=\"*50)\n\n# Show what the model confused with what\nif len(df_results) > 0:\n    wrong_preds = df_results[~df_results['is_correct']]\n    if len(wrong_preds) > 0:\n        print(\"\\nMisclassifications:\")\n        for idx, row in wrong_preds.iterrows():\n            print(f\"  {row['query_image'][:30]:<30} → Predicted: {row['predicted_species']:<30} (should be: {row['true_species']})\")\n            print(f\"    Top-5: {', '.join(row['top5_predictions'][:5])}\")\n        \n        # Check if true label appears in top-5\n        print(f\"\\n{len(wrong_preds)} misclassifications, but true label in top-5: {sum([row['true_species'] in row['top5_predictions'] for _, row in wrong_preds.iterrows()])} times\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T05:06:06.437442Z","iopub.execute_input":"2025-09-30T05:06:06.437918Z","iopub.status.idle":"2025-09-30T05:06:06.460237Z","shell.execute_reply.started":"2025-09-30T05:06:06.437862Z","shell.execute_reply":"2025-09-30T05:06:06.458245Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nCONFUSION ANALYSIS\n==================================================\n\nMisclassifications:\n  IFCB5_2014_248_004113_05455.pn → Predicted: Rhizosolenia                   (should be: DactFragCerataul)\n    Top-5: Rhizosolenia, Pseudonitzschia, G_delicatula_parasite, DactFragCerataul, G_delicatula_external_parasite\n  IFCB5_2014_315_144452_00017.pn → Predicted: Skeletonema                    (should be: DactFragCerataul)\n    Top-5: Skeletonema, DactFragCerataul, Cerataulina, Leptocylindrus, Amphidinium_sp\n  IFCB5_2014_315_144452_01507.pn → Predicted: Skeletonema                    (should be: DactFragCerataul)\n    Top-5: Skeletonema, Cerataulina, mix_elongated, Leptocylindrus, spore\n  IFCB5_2014_259_120213_05242.pn → Predicted: Rhizosolenia                   (should be: DactFragCerataul)\n    Top-5: Rhizosolenia, DactFragCerataul, Pseudonitzschia, G_delicatula_parasite, G_delicatula_external_parasite\n  IFCB5_2014_002_210221_01583.pn → Predicted: Rhizosolenia                   (should be: DactFragCerataul)\n    Top-5: Rhizosolenia, Pseudonitzschia, DactFragCerataul, G_delicatula_parasite, G_delicatula_external_parasite\n  IFCB5_2014_315_142115_01290.pn → Predicted: Rhizosolenia                   (should be: DactFragCerataul)\n    Top-5: Rhizosolenia, Pseudonitzschia, DactFragCerataul, G_delicatula_parasite, G_delicatula_external_parasite\n  IFCB5_2014_315_135823_07315.pn → Predicted: Skeletonema                    (should be: DactFragCerataul)\n    Top-5: Skeletonema, mix_elongated, Cerataulina, spore, Chaetoceros\n\n7 misclassifications, but true label in top-5: 5 times\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ---------------------------\n# Step 8. Test on Multiple Species\n# ---------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"TESTING ON MULTIPLE SPECIES\")\nprint(\"=\"*50)\n\n# Test on first 5 species to get better accuracy estimate\nall_test_results = []\n\nfor test_idx, species_folder in enumerate(species_folders[:5]):\n    test_images = list(species_folder.glob(\"*.png\"))[:10]\n    \n    if not test_images:\n        continue\n    \n    print(f\"\\nTesting {species_folder.name} ({len(test_images)} images)...\")\n    \n    for img_path in test_images:\n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n            \n            if CONFIG['image_preprocessing']:\n                img = preprocess_plankton_image(img)\n            \n            if CONFIG['augment_query']:\n                augmented = apply_augmentation(img)\n                embs = batch_image_embeddings(augmented)\n                q_emb = np.mean(embs, axis=0)\n                q_emb = q_emb / np.linalg.norm(q_emb)\n            else:\n                q_emb = image_embedding(img)\n            \n            similarities = ref_matrix @ q_emb\n            best_idx = np.argmax(similarities)\n            best_species = species_list[best_idx]\n            \n            top5_indices = np.argsort(similarities)[-5:][::-1]\n            top5_species = [species_list[i] for i in top5_indices]\n            \n            all_test_results.append({\n                \"true_species\": species_folder.name,\n                \"predicted_species\": best_species,\n                \"is_correct\": best_species == species_folder.name,\n                \"in_top5\": species_folder.name in top5_species\n            })\n            \n        except Exception as e:\n            continue\n\nif all_test_results:\n    df_all = pd.DataFrame(all_test_results)\n    overall_acc = df_all['is_correct'].mean()\n    overall_top5 = df_all['in_top5'].mean()\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"OVERALL PERFORMANCE (5 species × 10 images)\")\n    print(f\"{'='*50}\")\n    print(f\"Top-1 Accuracy: {overall_acc:.2%}\")\n    print(f\"Top-5 Accuracy: {overall_top5:.2%}\")\n    \n    # Per-species breakdown\n    print(\"\\nPer-species accuracy:\")\n    for species in df_all['true_species'].unique():\n        species_data = df_all[df_all['true_species'] == species]\n        species_acc = species_data['is_correct'].mean()\n        print(f\"  {species:<30}: {species_acc:.1%} ({species_data['is_correct'].sum()}/{len(species_data)})\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Pipeline complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T05:06:06.462270Z","iopub.execute_input":"2025-09-30T05:06:06.462752Z","iopub.status.idle":"2025-09-30T05:06:34.484930Z","shell.execute_reply.started":"2025-09-30T05:06:06.462720Z","shell.execute_reply":"2025-09-30T05:06:34.484042Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTESTING ON MULTIPLE SPECIES\n==================================================\n\nTesting DactFragCerataul (10 images)...\n\nTesting Rhizosolenia (10 images)...\n\nTesting Chaetoceros (10 images)...\n\nTesting bead (10 images)...\n\nTesting G_delicatula_external_parasite (9 images)...\n\n==================================================\nOVERALL PERFORMANCE (5 species × 10 images)\n==================================================\nTop-1 Accuracy: 61.22%\nTop-5 Accuracy: 81.63%\n\nPer-species accuracy:\n  DactFragCerataul              : 30.0% (3/10)\n  Rhizosolenia                  : 90.0% (9/10)\n  Chaetoceros                   : 50.0% (5/10)\n  bead                          : 90.0% (9/10)\n  G_delicatula_external_parasite: 44.4% (4/9)\n\n==================================================\nPipeline complete!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ---------------------------\n# Step 9. Analyze Relationship Between Sample Size and Accuracy\n# ---------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"SAMPLE SIZE vs ACCURACY ANALYSIS\")\nprint(\"=\"*50)\n\nif all_test_results:\n    # Merge test results with reference dataset info\n    species_performance = []\n    \n    for species in df_all['true_species'].unique():\n        # Get accuracy for this species\n        species_data = df_all[df_all['true_species'] == species]\n        species_acc = species_data['is_correct'].mean()\n        species_top5 = species_data['in_top5'].mean()\n        \n        # Get reference data for this species\n        ref_info = df_ref[df_ref['species'] == species]\n        \n        if len(ref_info) > 0:\n            species_performance.append({\n                'species': species,\n                'total_images_available': ref_info.iloc[0]['total_images_available'],\n                'num_reference_images_used': ref_info.iloc[0]['num_reference_images'],\n                'num_embeddings_used': ref_info.iloc[0]['num_embeddings_used'],\n                'top1_accuracy': species_acc,\n                'top5_accuracy': species_top5,\n                'num_test_images': len(species_data)\n            })\n    \n    df_performance = pd.DataFrame(species_performance)\n    df_performance = df_performance.sort_values('total_images_available')\n    \n    print(\"\\nSpecies Performance by Sample Size:\")\n    print(df_performance.to_string(index=False))\n    \n    # Calculate correlation\n    if len(df_performance) > 1:\n        # Handle NaN values in correlation calculation\n        if df_performance['top1_accuracy'].std() > 0 and df_performance['total_images_available'].std() > 0:\n            correlation = df_performance[['total_images_available', 'top1_accuracy']].corr().iloc[0, 1]\n            if not np.isnan(correlation):\n                print(f\"\\nCorrelation between sample size and accuracy: {correlation:.3f}\")\n            else:\n                print(\"\\nCorrelation could not be calculated (insufficient variation in data)\")\n        else:\n            print(\"\\nCorrelation could not be calculated (no variation in accuracy or sample size)\")\n        \n        # Categorize by sample size\n        print(\"\\nAccuracy by Sample Size Category:\")\n        \n        df_performance['size_category'] = pd.cut(\n            df_performance['total_images_available'],\n            bins=[0, 10, 50, 200, float('inf')],\n            labels=['Very Low (1-10)', 'Low (11-50)', 'Medium (51-200)', 'High (200+)']\n        )\n        \n        category_stats = df_performance.groupby('size_category', observed=True).agg({\n            'top1_accuracy': ['mean', 'std', 'count'],\n            'top5_accuracy': ['mean']\n        }).round(3)\n        \n        print(category_stats)\n        \n        # Find problematic species (low accuracy despite high samples)\n        print(\"\\nSpecies with surprisingly low accuracy (>50 images but <50% accuracy):\")\n        problematic = df_performance[\n            (df_performance['total_images_available'] > 50) & \n            (df_performance['top1_accuracy'] < 0.5)\n        ]\n        if len(problematic) > 0:\n            print(problematic[['species', 'total_images_available', 'num_reference_images_used', 'top1_accuracy']].to_string(index=False))\n        else:\n            print(\"  None found - good!\")\n        \n        # Find high performers with low samples\n        print(\"\\nSpecies with high accuracy despite low samples (<20 images but >60% accuracy):\")\n        high_performers = df_performance[\n            (df_performance['total_images_available'] < 20) & \n            (df_performance['top1_accuracy'] > 0.6)\n        ]\n        if len(high_performers) > 0:\n            print(high_performers[['species', 'total_images_available', 'num_reference_images_used', 'top1_accuracy']].to_string(index=False))\n        else:\n            print(\"  None found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T05:14:31.808584Z","iopub.execute_input":"2025-09-30T05:14:31.809190Z","iopub.status.idle":"2025-09-30T05:14:31.869645Z","shell.execute_reply.started":"2025-09-30T05:14:31.809146Z","shell.execute_reply":"2025-09-30T05:14:31.868397Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nSAMPLE SIZE vs ACCURACY ANALYSIS\n==================================================\n\nSpecies Performance by Sample Size:\n                       species  total_images_available  num_reference_images_used  num_embeddings_used  top1_accuracy  top5_accuracy  num_test_images\nG_delicatula_external_parasite                       9                          9                   54       0.444444       0.888889                9\n                          bead                      17                         17                  102       0.900000       0.900000               10\n              DactFragCerataul                     175                         50                  300       0.300000       0.800000               10\n                   Chaetoceros                    1871                         50                  300       0.500000       0.600000               10\n                  Rhizosolenia                    2199                         50                  300       0.900000       0.900000               10\n\nCorrelation between sample size and accuracy: 0.322\n\nAccuracy by Sample Size Category:\n                top1_accuracy              top5_accuracy\n                         mean    std count          mean\nsize_category                                           \nVery Low (1-10)         0.444    NaN     1         0.889\nLow (11-50)             0.900    NaN     1         0.900\nMedium (51-200)         0.300    NaN     1         0.800\nHigh (200+)             0.700  0.283     2         0.750\n\nSpecies with surprisingly low accuracy (>50 images but <50% accuracy):\n         species  total_images_available  num_reference_images_used  top1_accuracy\nDactFragCerataul                     175                         50            0.3\n\nSpecies with high accuracy despite low samples (<20 images but >60% accuracy):\nspecies  total_images_available  num_reference_images_used  top1_accuracy\n   bead                      17                         17            0.9\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":13}]}