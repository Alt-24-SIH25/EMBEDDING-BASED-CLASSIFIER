{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2331456,"sourceType":"datasetVersion","datasetId":1407240},{"sourceId":8815412,"sourceType":"datasetVersion","datasetId":5302658}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T17:41:44.958551Z","iopub.execute_input":"2025-09-29T17:41:44.959392Z","iopub.status.idle":"2025-09-29T17:42:18.206561Z","shell.execute_reply.started":"2025-09-29T17:41:44.959357Z","shell.execute_reply":"2025-09-29T17:42:18.205785Z"}},"outputs":[{"name":"stderr","text":"2025-09-29 17:41:59.199163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759167719.443981      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759167719.515776      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ---------------------------\n# Configuration\n# ---------------------------\nCONFIG = {\n    'num_reference_images': 50, \n    'embedding_method': 'mean',\n    'augment_reference': True,\n    'augment_query': True, \n    'image_preprocessing': True,\n}\n\n# ---------------------------\n# Step 1. Setup paths and explore dataset\n# ---------------------------\ndataset_path = Path(\"/kaggle/input/whoi-plankton-2014/2014\")\n\n# List species folders\nspecies_folders = [p for p in dataset_path.iterdir() if p.is_dir()]\nprint(f\"Number of species: {len(species_folders)}\")\nprint(f\"Example species: {[f.name for f in species_folders[:5]]}\")\n\n# ---------------------------\n# Step 2. Image Preprocessing Functions\n# ---------------------------\ndef preprocess_plankton_image(pil_img: Image.Image, target_size=224) -> Image.Image:\n    \"\"\"\n    Preprocess plankton images for better CLIP performance\n    - Resize while maintaining aspect ratio\n    - Add padding to make square\n    - Enhance contrast\n    \"\"\"\n    # Convert to RGB\n    img = pil_img.convert(\"RGB\")\n    \n    # Get original size\n    orig_w, orig_h = img.size\n    \n    # Calculate scaling to fit within target_size while maintaining aspect ratio\n    scale = min(target_size / orig_w, target_size / orig_h)\n    new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n    \n    # Resize image\n    img = img.resize((new_w, new_h), Image.LANCZOS)\n    \n    # Create white background (plankton images often have white/light backgrounds)\n    background = Image.new('RGB', (target_size, target_size), (255, 255, 255))\n    \n    # Paste resized image in center\n    offset = ((target_size - new_w) // 2, (target_size - new_h) // 2)\n    background.paste(img, offset)\n    \n    return background\n\ndef apply_augmentation(pil_img: Image.Image) -> list:\n    \"\"\"\n    Apply augmentations to generate multiple views of the same image\n    Returns list of augmented images\n    \"\"\"\n    augmented = [pil_img]  # Original\n    \n    # Horizontal flip\n    augmented.append(pil_img.transpose(Image.FLIP_LEFT_RIGHT))\n    \n    # Vertical flip\n    augmented.append(pil_img.transpose(Image.FLIP_TOP_BOTTOM))\n    \n    # Slight rotations\n    augmented.append(pil_img.rotate(90, expand=True))\n    augmented.append(pil_img.rotate(180, expand=True))\n    augmented.append(pil_img.rotate(270, expand=True))\n    \n    return augmented\n\n# ---------------------------\n# Step 3. CLIP setup\n# ---------------------------\nmodel_path = \"/kaggle/input/openaiclip-vit-base-patch32\"\n\nprint(\"Loading CLIP model...\")\nclip_model = CLIPModel.from_pretrained(model_path)\nclip_processor = CLIPProcessor.from_pretrained(model_path)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model = clip_model.to(DEVICE)\nprint(f\"Using device: {DEVICE}\")\n\n@torch.no_grad()\ndef image_embedding(pil_img: Image.Image) -> np.ndarray:\n    \"\"\"Extract CLIP embedding from PIL image\"\"\"\n    inputs = clip_processor(images=pil_img, return_tensors=\"pt\").to(DEVICE)\n    feats = clip_model.get_image_features(**inputs).cpu().numpy()\n    feats = feats / np.linalg.norm(feats, axis=1, keepdims=True)\n    return feats[0]\n\n@torch.no_grad()\ndef batch_image_embeddings(pil_images: list) -> np.ndarray:\n    \"\"\"Extract CLIP embeddings for multiple images at once (faster)\"\"\"\n    inputs = clip_processor(images=pil_images, return_tensors=\"pt\").to(DEVICE)\n    feats = clip_model.get_image_features(**inputs).cpu().numpy()\n    feats = feats / np.linalg.norm(feats, axis=1, keepdims=True)\n    return feats\n\n# ---------------------------\n# Step 4. Build IMPROVED reference dataset\n# ---------------------------\nprint(f\"Using {CONFIG['num_reference_images']} images per species\")\nprint(f\"Augmentation: {CONFIG['augment_reference']}\")\nprint(f\"Preprocessing: {CONFIG['image_preprocessing']}\")\n\nspecies_embeddings = {}\nrecords = []\n\nfor species_folder in tqdm(sorted(dataset_path.iterdir()), desc=\"Processing species\"):\n    if not species_folder.is_dir():\n        continue\n    \n    species_name = species_folder.name\n    png_files = list(species_folder.glob(\"*.png\"))\n    \n    if not png_files:\n        print(f\"Warning: No PNG files found in {species_name}\")\n        continue\n    \n    # Select multiple reference images\n    num_refs = min(CONFIG['num_reference_images'], len(png_files))\n    \n    # Distribute selection across the dataset (not just first N)\n    if num_refs >= len(png_files):\n        # Use all images if we have fewer than requested\n        selected_files = png_files\n    else:\n        # Sample evenly distributed images\n        step = max(1, len(png_files) // num_refs)\n        selected_files = png_files[::step][:num_refs]\n    \n    all_embeddings = []\n    \n    try:\n        for img_path in selected_files:\n            img = Image.open(img_path).convert(\"RGB\")\n            \n            # Apply preprocessing\n            if CONFIG['image_preprocessing']:\n                img = preprocess_plankton_image(img)\n            \n            # Apply augmentation if enabled\n            if CONFIG['augment_reference']:\n                augmented_images = apply_augmentation(img)\n                # Get embeddings for all augmented versions\n                embs = batch_image_embeddings(augmented_images)\n                all_embeddings.extend(embs)\n            else:\n                emb = image_embedding(img)\n                all_embeddings.append(emb)\n        \n        # Aggregate embeddings \n        all_embeddings = np.array(all_embeddings)\n        \n        if CONFIG['embedding_method'] == 'mean':\n            final_embedding = np.mean(all_embeddings, axis=0)\n        else:  # median\n            final_embedding = np.median(all_embeddings, axis=0)\n        \n        # Renormalize\n        final_embedding = final_embedding / np.linalg.norm(final_embedding)\n        \n        species_embeddings[species_name] = final_embedding\n        \n        records.append({\n            \"species\": species_name,\n            \"num_reference_images\": num_refs,\n            \"num_embeddings_used\": len(all_embeddings),\n            \"total_images_available\": len(png_files),\n            \"embedding\": final_embedding\n        })\n        \n    except Exception as e:\n        print(f\"Error processing {species_name}: {e}\")\n        continue\n\ndf_ref = pd.DataFrame(records)\nprint(f\"\\nReference dataset built: {df_ref.shape}\")\nprint(f\"Species with embeddings: {len(species_embeddings)}\")\nprint(\"\\nEmbeddings statistics:\")\nprint(df_ref[['species', 'num_reference_images', 'num_embeddings_used', 'total_images_available']].head(10))\n\n# ---------------------------\n# Step 5. Classify test images with improvements\n# ---------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"Testing classification\")\nprint(\"=\"*50)\n\ntest_species_folder = species_folders[0]\ntest_images = list(test_species_folder.glob(\"*.png\"))[:10]\n\nprint(f\"\\nTest species: {test_species_folder.name}\")\nprint(f\"Found {len(test_images)} test images\")\n\nspecies_list = list(species_embeddings.keys())\nref_matrix = np.stack([species_embeddings[s] for s in species_list])\n\nresults = []\n\nfor img_path in tqdm(test_images, desc=\"Classifying images\"):\n    try:\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        # Apply preprocessing\n        if CONFIG['image_preprocessing']:\n            img = preprocess_plankton_image(img)\n        \n        # Test-time augmentation \n        if CONFIG['augment_query']:\n            augmented = apply_augmentation(img)\n            embs = batch_image_embeddings(augmented)\n            q_emb = np.mean(embs, axis=0)\n            q_emb = q_emb / np.linalg.norm(q_emb)\n        else:\n            q_emb = image_embedding(img)\n        \n        # Compute similarities\n        similarities = ref_matrix @ q_emb\n        \n        # Get predictions\n        best_idx = np.argmax(similarities)\n        best_species = species_list[best_idx]\n        best_score = similarities[best_idx]\n        \n        # Top 5 predictions\n        top5_indices = np.argsort(similarities)[-5:][::-1]\n        top5_species = [species_list[i] for i in top5_indices]\n        top5_scores = [similarities[i] for i in top5_indices]\n        \n        results.append({\n            \"query_image\": img_path.name,\n            \"true_species\": test_species_folder.name,\n            \"predicted_species\": best_species,\n            \"confidence\": float(best_score),\n            \"is_correct\": best_species == test_species_folder.name,\n            \"top5_predictions\": top5_species,\n            \"top5_scores\": [float(s) for s in top5_scores]\n        })\n        \n    except Exception as e:\n        print(f\"Error processing {img_path.name}: {e}\")\n        continue\n\ndf_results = pd.DataFrame(results)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Classification Results\")\nprint(\"=\"*50)\nprint(df_results[['query_image', 'true_species', 'predicted_species', 'confidence', 'is_correct']])\n\nif len(df_results) > 0:\n    accuracy = df_results['is_correct'].mean()\n    avg_confidence = df_results['confidence'].mean()\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"METRICS\")\n    print(f\"{'='*50}\")\n    print(f\"Accuracy: {accuracy:.2%} ({df_results['is_correct'].sum()}/{len(df_results)})\")\n    print(f\"Average confidence: {avg_confidence:.4f}\")\n    print(f\"Correct predictions avg confidence: {df_results[df_results['is_correct']]['confidence'].mean():.4f}\")\n    print(f\"Wrong predictions avg confidence: {df_results[~df_results['is_correct']]['confidence'].mean():.4f}\")\n    \n    # Top-5 accuracy\n    top5_correct = sum([row['true_species'] in row['top5_predictions'] for _, row in df_results.iterrows()])\n    top5_accuracy = top5_correct / len(df_results)\n    print(f\"Top-5 Accuracy: {top5_accuracy:.2%}\")\n\n# ---------------------------\n# Step 6. Save results\n# ---------------------------\ndf_ref_save = df_ref.drop(columns=['embedding'])\ndf_ref_save.to_csv(\"reference_dataset_improved.csv\", index=False)\n\ndf_results_save = df_results.drop(columns=['top5_predictions', 'top5_scores'], errors='ignore')\ndf_results_save.to_csv(\"classification_results_improved.csv\", index=False)\n\nembeddings_array = np.stack([species_embeddings[s] for s in species_list])\nnp.save(\"species_embeddings_improved.npy\", embeddings_array)\nnp.save(\"species_names.npy\", np.array(species_list))\n\n# ---------------------------\n# Step 7. Analyze Confusion Patterns\n# ---------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"CONFUSION ANALYSIS\")\nprint(\"=\"*50)\n\n# Show what the model confused with what\nif len(df_results) > 0:\n    wrong_preds = df_results[~df_results['is_correct']]\n    if len(wrong_preds) > 0:\n        print(\"\\nMisclassifications:\")\n        for idx, row in wrong_preds.iterrows():\n            print(f\"  {row['query_image'][:30]:<30} → Predicted: {row['predicted_species']:<30} (should be: {row['true_species']})\")\n            print(f\"    Top-5: {', '.join(row['top5_predictions'][:5])}\")\n        \n        # Check if true label appears in top-5\n        print(f\"\\n{len(wrong_preds)} misclassifications, but true label in top-5: {sum([row['true_species'] in row['top5_predictions'] for _, row in wrong_preds.iterrows()])} times\")\n\n# ---------------------------\n# Step 8. Test on Multiple Species\n# ---------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"TESTING ON MULTIPLE SPECIES\")\nprint(\"=\"*50)\n\n# Test on first 5 species to get better accuracy estimate\nall_test_results = []\n\nfor test_idx, species_folder in enumerate(species_folders[:5]):\n    test_images = list(species_folder.glob(\"*.png\"))[:10]\n    \n    if not test_images:\n        continue\n    \n    print(f\"\\nTesting {species_folder.name} ({len(test_images)} images)...\")\n    \n    for img_path in test_images:\n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n            \n            if CONFIG['image_preprocessing']:\n                img = preprocess_plankton_image(img)\n            \n            if CONFIG['augment_query']:\n                augmented = apply_augmentation(img)\n                embs = batch_image_embeddings(augmented)\n                q_emb = np.mean(embs, axis=0)\n                q_emb = q_emb / np.linalg.norm(q_emb)\n            else:\n                q_emb = image_embedding(img)\n            \n            similarities = ref_matrix @ q_emb\n            best_idx = np.argmax(similarities)\n            best_species = species_list[best_idx]\n            \n            top5_indices = np.argsort(similarities)[-5:][::-1]\n            top5_species = [species_list[i] for i in top5_indices]\n            \n            all_test_results.append({\n                \"true_species\": species_folder.name,\n                \"predicted_species\": best_species,\n                \"is_correct\": best_species == species_folder.name,\n                \"in_top5\": species_folder.name in top5_species\n            })\n            \n        except Exception as e:\n            continue\n\nif all_test_results:\n    df_all = pd.DataFrame(all_test_results)\n    overall_acc = df_all['is_correct'].mean()\n    overall_top5 = df_all['in_top5'].mean()\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"OVERALL PERFORMANCE (5 species × 10 images)\")\n    print(f\"{'='*50}\")\n    print(f\"Top-1 Accuracy: {overall_acc:.2%}\")\n    print(f\"Top-5 Accuracy: {overall_top5:.2%}\")\n    \n    # Per-species breakdown\n    print(\"\\nPer-species accuracy:\")\n    for species in df_all['true_species'].unique():\n        species_data = df_all[df_all['true_species'] == species]\n        species_acc = species_data['is_correct'].mean()\n        print(f\"  {species:<30}: {species_acc:.1%} ({species_data['is_correct'].sum()}/{len(species_data)})\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Pipeline complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:02:45.215375Z","iopub.execute_input":"2025-09-29T19:02:45.215709Z","iopub.status.idle":"2025-09-29T19:26:24.089242Z","shell.execute_reply.started":"2025-09-29T19:02:45.215688Z","shell.execute_reply":"2025-09-29T19:26:24.087990Z"}},"outputs":[{"name":"stdout","text":"Number of species: 94\nExample species: ['DactFragCerataul', 'Rhizosolenia', 'Chaetoceros', 'bead', 'G_delicatula_external_parasite']\nLoading CLIP model...\nUsing device: cpu\nUsing 50 images per species\nAugmentation: True\nPreprocessing: True\n","output_type":"stream"},{"name":"stderr","text":"Processing species: 100%|██████████| 94/94 [23:07<00:00, 14.76s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nReference dataset built: (94, 5)\nSpecies with embeddings: 94\n\nEmbeddings statistics:\n                          species  num_reference_images  num_embeddings_used  \\\n0                        Akashiwo                     1                    6   \n1                  Amphidinium_sp                    50                  300   \n2                Asterionellopsis                    50                  300   \n3                     Cerataulina                    50                  300   \n4          Cerataulina_flagellate                     5                   30   \n5                        Ceratium                     6                   36   \n6                     Chaetoceros                    50                  300   \n7             Chaetoceros_didymus                    11                   66   \n8  Chaetoceros_didymus_flagellate                     1                    6   \n9          Chaetoceros_flagellate                     4                   24   \n\n   total_images_available  \n0                       1  \n1                      66  \n2                     128  \n3                     412  \n4                       5  \n5                       6  \n6                    1871  \n7                      11  \n8                       1  \n9                       4  \n\n==================================================\nTesting classification\n==================================================\n\nTest species: DactFragCerataul\nFound 10 test images\n","output_type":"stream"},{"name":"stderr","text":"Classifying images: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nClassification Results\n==================================================\n                       query_image      true_species predicted_species  \\\n0  IFCB5_2014_248_004113_05455.png  DactFragCerataul      Rhizosolenia   \n1  IFCB5_2014_328_152205_07231.png  DactFragCerataul  DactFragCerataul   \n2  IFCB5_2014_315_144452_00017.png  DactFragCerataul       Skeletonema   \n3  IFCB5_2014_315_135823_06074.png  DactFragCerataul  DactFragCerataul   \n4  IFCB5_2014_315_144452_01507.png  DactFragCerataul       Skeletonema   \n5  IFCB5_2014_259_120213_05242.png  DactFragCerataul      Rhizosolenia   \n6  IFCB5_2014_002_210221_01583.png  DactFragCerataul      Rhizosolenia   \n7  IFCB5_2014_315_142115_01290.png  DactFragCerataul      Rhizosolenia   \n8  IFCB5_2014_248_010423_01317.png  DactFragCerataul  DactFragCerataul   \n9  IFCB5_2014_315_135823_07315.png  DactFragCerataul       Skeletonema   \n\n   confidence  is_correct  \n0    0.974654       False  \n1    0.972635        True  \n2    0.957793       False  \n3    0.953981        True  \n4    0.984811       False  \n5    0.966115       False  \n6    0.987965       False  \n7    0.982660       False  \n8    0.966641        True  \n9    0.980332       False  \n\n==================================================\nMETRICS\n==================================================\nAccuracy: 30.00% (3/10)\nAverage confidence: 0.9728\nCorrect predictions avg confidence: 0.9644\nWrong predictions avg confidence: 0.9763\nTop-5 Accuracy: 80.00%\n\n==================================================\nCONFUSION ANALYSIS\n==================================================\n\nMisclassifications:\n  IFCB5_2014_248_004113_05455.pn → Predicted: Rhizosolenia                   (should be: DactFragCerataul)\n    Top-5: Rhizosolenia, Pseudonitzschia, G_delicatula_parasite, DactFragCerataul, G_delicatula_external_parasite\n  IFCB5_2014_315_144452_00017.pn → Predicted: Skeletonema                    (should be: DactFragCerataul)\n    Top-5: Skeletonema, DactFragCerataul, Cerataulina, Leptocylindrus, Amphidinium_sp\n  IFCB5_2014_315_144452_01507.pn → Predicted: Skeletonema                    (should be: DactFragCerataul)\n    Top-5: Skeletonema, Cerataulina, mix_elongated, Leptocylindrus, spore\n  IFCB5_2014_259_120213_05242.pn → Predicted: Rhizosolenia                   (should be: DactFragCerataul)\n    Top-5: Rhizosolenia, DactFragCerataul, Pseudonitzschia, G_delicatula_parasite, G_delicatula_external_parasite\n  IFCB5_2014_002_210221_01583.pn → Predicted: Rhizosolenia                   (should be: DactFragCerataul)\n    Top-5: Rhizosolenia, Pseudonitzschia, DactFragCerataul, G_delicatula_parasite, G_delicatula_external_parasite\n  IFCB5_2014_315_142115_01290.pn → Predicted: Rhizosolenia                   (should be: DactFragCerataul)\n    Top-5: Rhizosolenia, Pseudonitzschia, DactFragCerataul, G_delicatula_parasite, G_delicatula_external_parasite\n  IFCB5_2014_315_135823_07315.pn → Predicted: Skeletonema                    (should be: DactFragCerataul)\n    Top-5: Skeletonema, mix_elongated, Cerataulina, spore, Chaetoceros\n\n7 misclassifications, but true label in top-5: 5 times\n\n==================================================\nTESTING ON MULTIPLE SPECIES\n==================================================\n\nTesting DactFragCerataul (10 images)...\n\nTesting Rhizosolenia (10 images)...\n\nTesting Chaetoceros (10 images)...\n\nTesting bead (10 images)...\n\nTesting G_delicatula_external_parasite (9 images)...\n\n==================================================\nOVERALL PERFORMANCE (5 species × 10 images)\n==================================================\nTop-1 Accuracy: 61.22%\nTop-5 Accuracy: 81.63%\n\nPer-species accuracy:\n  DactFragCerataul              : 30.0% (3/10)\n  Rhizosolenia                  : 90.0% (9/10)\n  Chaetoceros                   : 50.0% (5/10)\n  bead                          : 90.0% (9/10)\n  G_delicatula_external_parasite: 44.4% (4/9)\n\n==================================================\nPipeline complete!\n","output_type":"stream"}],"execution_count":19}]}